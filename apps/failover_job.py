#!/usr/bin/env python3
"""
Job Spark avec m√©canisme de failover et auto-restart
POC pour d√©monstration Docker Compose
"""

import os
import time
import random
import logging
import signal
import sys
from datetime import datetime
from typing import Optional

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, avg, max as spark_max
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/logs/spark_app.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class SparkFailoverJob:
    def __init__(self):
        self.spark: Optional[SparkSession] = None
        self.running = False
        self.restart_count = 0
        self.max_restarts = 5
        self.failure_rate = 0.3  # 30% de chance de panne
        
    def create_spark_session(self) -> bool:
        """Cr√©er une session Spark avec configuration optimis√©e"""
        try:
            self.spark = SparkSession.builder \
                .appName("FailoverPOC") \
                .master(os.getenv("SPARK_MASTER_URL", "local[*]")) \
                .config("spark.sql.adaptive.enabled", "true") \
                .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
                .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
                .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
                .config("spark.sql.adaptive.skewJoin.enabled", "true") \
                .config("spark.dynamicAllocation.enabled", "false") \
                .config("spark.sql.streaming.checkpointLocation", "/data/checkpoints") \
                .getOrCreate()
            
            # Configuration du niveau de log
            self.spark.sparkContext.setLogLevel("WARN")
            
            logger.info("‚úì Session Spark cr√©√©e avec succ√®s")
            return True
            
        except Exception as e:
            logger.error(f"‚úó Erreur cr√©ation session Spark: {e}")
            return False
    
    def generate_sample_data(self):
        """G√©n√©rer des donn√©es d'exemple pour le traitement"""
        try:
            # Donn√©es d'exemple : commandes e-commerce
            orders_data = []
            for i in range(1000):
                orders_data.append({
                    'order_id': f'ORD_{i:04d}',
                    'customer_id': f'CUST_{random.randint(1, 100):03d}',
                    'product_category': random.choice(['Electronics', 'Clothing', 'Books', 'Home']),
                    'amount': round(random.uniform(10.0, 500.0), 2),
                    'timestamp': datetime.now().isoformat()
                })
            
            # Cr√©er le DataFrame
            schema = StructType([
                StructField("order_id", StringType(), True),
                StructField("customer_id", StringType(), True),
                StructField("product_category", StringType(), True),
                StructField("amount", IntegerType(), True),
                StructField("timestamp", StringType(), True)
            ])
            
            df = self.spark.createDataFrame(orders_data)
            
            # Sauvegarder les donn√©es sources
            df.write \
              .mode("overwrite") \
              .option("header", "true") \
              .csv("/data/input/orders")
            
            logger.info(f"‚úì Donn√©es g√©n√©r√©es: {df.count()} commandes")
            return df
            
        except Exception as e:
            logger.error(f"‚úó Erreur g√©n√©ration donn√©es: {e}")
            raise
    
    def process_data(self, df):
        """Traiter les donn√©es avec possibilit√© de panne"""
        try:
            # Simuler une panne al√©atoire
            if random.random() < self.failure_rate:
                raise Exception("Panne simul√©e du traitement de donn√©es")
            
            # Traitement des donn√©es
            logger.info("D√©but du traitement des donn√©es...")
            
            # Analyses par cat√©gorie
            category_analysis = df.groupBy("product_category") \
                                 .agg(
                                     count("*").alias("total_orders"),
                                     avg("amount").alias("avg_amount"),
                                     spark_max("amount").alias("max_amount")
                                 ) \
                                 .orderBy("total_orders", ascending=False)
            
            # Analyses par client
            customer_analysis = df.groupBy("customer_id") \
                                 .agg(
                                     count("*").alias("total_orders"),
                                     avg("amount").alias("avg_amount")
                                 ) \
                                 .filter(col("total_orders") > 5) \
                                 .orderBy("total_orders", ascending=False)
            
            # Sauvegarder les r√©sultats
            category_analysis.write \
                            .mode("overwrite") \
                            .option("header", "true") \
                            .csv("/data/output/category_analysis")
            
            customer_analysis.write \
                            .mode("overwrite") \
                            .option("header", "true") \
                            .csv("/data/output/customer_analysis")
            
            logger.info("‚úì Traitement termin√© avec succ√®s")
            logger.info(f"  - Analyses par cat√©gorie: {category_analysis.count()} lignes")
            logger.info(f"  - Analyses par client: {customer_analysis.count()} lignes")
            
            return True
            
        except Exception as e:
            logger.error(f"‚úó Erreur traitement: {e}")
            raise
    
    def health_check(self) -> bool:
        """V√©rifier l'√©tat de sant√© de Spark"""
        try:
            if not self.spark:
                return False
                
            # Test simple de connectivit√©
            test_df = self.spark.range(1)
            test_df.count()
            return True
            
        except Exception:
            return False
    
    def cleanup(self):
        """Nettoyer les ressources Spark"""
        try:
            if self.spark:
                self.spark.stop()
                self.spark = None
                logger.info("‚úì Session Spark nettoy√©e")
        except Exception as e:
            logger.error(f"‚úó Erreur nettoyage: {e}")
    
    def run_with_failover(self):
        """Ex√©cuter le job avec m√©canisme de failover"""
        self.running = True
        
        logger.info("üöÄ D√©marrage du job Spark avec failover")
        
        while self.running and self.restart_count < self.max_restarts:
            try:
                # Cr√©er la session Spark si n√©cessaire
                if not self.spark or not self.health_check():
                    self.cleanup()
                    if not self.create_spark_session():
                        raise Exception("Impossible de cr√©er la session Spark")
                
                logger.info(f"üîÑ Ex√©cution #{self.restart_count + 1}")
                
                # G√©n√©rer et traiter les donn√©es
                df = self.generate_sample_data()
                self.process_data(df)
                
                # R√©initialiser le compteur de red√©marrage en cas de succ√®s
                self.restart_count = 0
                
                # Attendre avant le prochain cycle
                logger.info("‚è≥ Attente avant le prochain cycle...")
                time.sleep(30)
                
            except Exception as e:
                logger.error(f"üí• Erreur dans le job: {e}")
                
                self.cleanup()
                self.restart_count += 1
                
                if self.restart_count < self.max_restarts:
                    wait_time = min(2 ** self.restart_count, 60)  # Backoff avec limite
                    logger.info(f"üîÑ Red√©marrage dans {wait_time}s (tentative {self.restart_count}/{self.max_restarts})")
                    time.sleep(wait_time)
                else:
                    logger.error("‚ùå Nombre maximum de red√©marrages atteint")
                    self.running = False
                    break
        
        self.cleanup()
        logger.info("üèÅ Job termin√©")

def signal_handler(signum, frame):
    """Gestionnaire pour arr√™t propre"""
    logger.info("üì° Signal d'arr√™t re√ßu")
    global job
    if job:
        job.running = False
        job.cleanup()
    sys.exit(0)

def main():
    """Fonction principale"""
    global job
    
    # Cr√©er les r√©pertoires n√©cessaires
    os.makedirs("/data/input", exist_ok=True)
    os.makedirs("/data/output", exist_ok=True)
    os.makedirs("/data/checkpoints", exist_ok=True)
    os.makedirs("/logs", exist_ok=True)
    
    # Installer les gestionnaires de signaux
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # D√©marrer le job
    job = SparkFailoverJob()
    
    try:
        job.run_with_failover()
    except KeyboardInterrupt:
        logger.info("üõë Arr√™t demand√© par l'utilisateur")
    except Exception as e:
        logger.error(f"üí• Erreur critique: {e}")
    finally:
        job.cleanup()

if __name__ == "__main__":
    main()